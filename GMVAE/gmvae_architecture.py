import os
import sys
import numpy as np
import matplotlib.pyplot as plt

# !pip3 install torch torchvision
import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

import wandb

"""
First step: reproduce architecture as described in the paper DOI: 10.1016/j.anucene.2024.110496

TODO:
 - for now, fully supervised
 - implement loss function for semi-supervised case (need to add k_NN classification)
"""

#---------------------------------
# Encoder paper:
# The encoder is designed to understand and learn the distinct features of the input pulses.
# It is built with three fully-connected layers with respective sizes of ùêø, ùêø‚àï2, and ùêø‚àï4, each utilizing a rectified linear unit (ReLU) for activation.
#        
# Latent Space paper:
# In the latent space, there is a classification module with a layer that is as large as the number of classes.
# This module analyzes the learned features from the encoder to allocate each data point to a predicted class based on its characteristics.
# Following this classification, it further refines the data representation by determining the mean and variance of the latent features, 
# a process that integrates information from the predicted classes and previous layers.     
#
# Decoder paper:
# Finally, the decoder part of the architecture aims to reconstruct the output based on the refined data representations in the latent space.
# It consists of three fully-connected layers with respective sizes of ùêø‚àï4, ùêø‚àï2, and ùêø, where the initial two layers use a ReLU activation function,
# and the last layer employs a logistic (sigmoid) function to deliver the final output vector.
#---------------------------------

class Encoder(nn.Module):
    def __init__(self, L, z_dim, n_classes):
        """
        L: length of the waveform (trigger window)
        :param z_dim: latent dimension (size of mu and logvar vectors)
        :param n_classes: number of classes (here, 3: neutron, gamma or pileup)
        """
        super().__init__()
        # 3 Layers: one input of size L, two hidden of sizes L/2, L/4
        self.fc1 = nn.Linear(L, L // 2)
        self.fc2 = nn.Linear(L // 2, L // 4)

        self.classifier = nn.Linear(L // 4, n_classes)
        self.mu = nn.Linear(L // 4, z_dim) # mean of the latent distribution
        self.logvar = nn.Linear(L // 4, z_dim) # Log Variance (log sigma^2) of the latent distribution
        # classifier decides the class
        # mu decides the position in the latent space
        # logvar decides the uncertainty (size) of that location

    def forward(self, x):
        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))

        logits = self.classifier(h)
        mu = self.mu(h)
        logvar = self.logvar(h)

        return logits, mu, logvar

class Decoder(nn.Module):
    def __init__(self, L, z_dim):
        super().__init__()
        self.fc1 = nn.Linear(z_dim, L // 4)
        self.fc2 = nn.Linear(L // 4, L // 2)
        self.fc3 = nn.Linear(L // 2, L)

    def forward(self, z):
        h = F.relu(self.fc1(z))
        h = F.relu(self.fc2(h))
        return torch.sigmoid(self.fc3(h))

#---------------------------------
#       Wrapper
#---------------------------------
class GMVAE(nn.Module):
    def __init__(self, L, z_dim=8, n_classes=3):
        super().__init__()
        self.encoder = Encoder(L, z_dim, n_classes)
        self.decoder = Decoder(L, z_dim)

    def reparameterize(self, mu, logvar):
        # add randomness while keeping the model differentiable
        std = torch.exp(0.5 * logvar) # Convert log variance to standard deviation
        eps = torch.randn_like(std) # Sample random noise (epsilon) - this not related to the Gaussian noise added when generating the datasets
        return mu + eps * std # latent vector z

    def forward(self, x):
        logits, mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decoder(z)
        return x_hat, logits, mu, logvar
    
#---------------------------------
#       Loss Function
#---------------------------------
def gmv_loss(x, x_hat, logits, mu, logvar, labels=None, alpha=50):
    """
    Loss function comports several parts:

    Total Loss} = MSE (Reconstruction) + KL Divergence + alpha * Cross Entropy

    recon: reconstruction loss. 
           Mean Square Error between the original input and the reconstructed one (the output generated by the decoder)
           Measures how well the decoder can recreate the input waveform from the latent vector.
    kl: KL divergence loss. Regularizer for the latent space. 
        Forces the learned latent distribution (defined by mu and logvar) to stay close to a Standard Normal Distribution ( mu = 0, sigma = 1). 
        This prevents the encoder from simply memorizing specific points and creates a smooth, continuous space for signal generation.¬†
        Ensures the latent space is well-organized and follows a standard normal distribution
    ce: cross entropy loss. Standard supervised cross-entropy used to predict the signal type (neutron, gamma, or pileup)

    hyperparamerter:
    alpha: controls the trade-off between the generative task and the classification task.
            A high value (like 50) indicates that the model prioritizes classification accuracy over perfect reconstruction or latent space smoothness. 
            If the model is classifying well but reconstructing poorly, you might lower alpha.

        returns 
        loss funcion
        seperate components of the loss (useful for sanity checks and to understand the model)
    """
    reco = F.mse_loss(x_hat, x, reduction='mean') # reconstruction loss (Mean Square Error)
    kl = -0.5 * torch.mean(1 + logvar - mu**2 - logvar.exp()) # KL divergence loss
    ce = F.cross_entropy(logits, labels)

    loss = reco + kl + (alpha * ce)

    return loss, reco, kl, ce

#---------------------------------
#       Training
#---------------------------------

def train_gmv(model, dataloader, optimizer, device, alpha=50):
    model.train()
    total = 0

    for x, y in dataloader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        x_hat, logits, mu, logvar = model(x)

        loss, _, _, _ = gmv_loss(x, x_hat, logits, mu, logvar, y, alpha)
        loss.backward()
        optimizer.step()

        total += loss.item()

    return total / len(dataloader)

def train_gmv_log(model, dataloader, optimizer, device, alpha=50):
    """
    Same as train_gmv but also returns the individual components of the loss
    """

    model.train()
    total_loss, total_reco, total_kl, total_ce = 0, 0, 0, 0

    for x, y in dataloader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        x_hat, logits, mu, logvar = model(x)
        
        # Calculate individual parts of the loss for logging
        loss, reco, kl, ce = gmv_loss(x, x_hat, logits, mu, logvar, y, alpha)
        
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_reco += reco.item()
        total_kl += kl.item()
        total_ce += ce.item()

    n = len(dataloader)
    return total_loss/n, total_reco/n, total_kl/n, total_ce/n

#----------------------------------------------------------
#       Training and logging with W&B
#----------------------------------------------------------

def train_epoch_wandb(model, dataloader, optimizer, device, alpha=50):
    """
     training function that logs metrics at every epoch
    """
    model.train()
    running_loss, running_recon, running_kl, running_ce = 0.0, 0.0, 0.0, 0.0
    correct, total = 0, 0

    for x, y in dataloader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        
        x_hat, logits, mu, logvar = model(x)
        
        # Loss components
        recon = F.mse_loss(x_hat, x, reduction='mean')
        kl = -0.5 * torch.mean(1 + logvar - mu**2 - logvar.exp())
        ce = F.cross_entropy(logits, y)
        loss = recon + kl + (alpha * ce)
        
        loss.backward()
        optimizer.step()

        # Metrics for classification accuracy
        _, predicted = torch.max(logits.data, 1)
        total += y.size(0)
        correct += (predicted == y).sum().item()

        running_loss += loss.item()
        running_recon += recon.item()
        running_kl += kl.item()
        running_ce += ce.item()

    n = len(dataloader)
    # Log to W&B
    wandb.log({
        "epoch_loss": running_loss / n,
        "recon_loss": running_recon / n,
        "kl_div": running_kl / n,
        "ce_loss": running_ce / n,
        "train_acc": 100 * correct / total
    })


# def log_visualizations(model, dataloader, device, epoch):
    # model.eval()
    # all_mu, all_labels = [], []
    
    # with torch.no_grad():
    #     x, y = next(iter(dataloader))
    #     x = x.to(device)
    #     x_hat, _, mu, _ = model(x)
        
    #     # # Log Reconstruction Comparison
    #     # fig, ax = plt.subplots(1, 2)
    #     # ax[0].plot(x[0].cpu().numpy(), label="Original")
    #     # ax[1].plot(x_hat[0].cpu().numpy(), label="Reconstructed", color='orange')
    #     # wandb.log({"reconstruction_sample": wandb.Image(plt)})
    #     # plt.close()

    #     # Log Latent Space (t-SNE)
    #     tsne = TSNE(n_components=2).fit_transform(mu.cpu().numpy())
    #     plt.figure(figsize=(8,6))
    #     scatter = plt.scatter(tsne[:, 0], tsne[:, 1], c=y.numpy(), cmap='viridis', alpha=0.8)
    #     plt.colorbar(scatter, ticks=[0, 1, 2], label="0:Gamma, 1:Neutron, 2:Pile-up")
    #     plt.title(f"t-SNE Latent Space (Epoch {epoch})")
    #     plt.xlabel("t-SNE dimension 1")
    #     plt.ylabel("t-SNE dimension 2")
    #     wandb.log({"latent_space": wandb.Image(plt)})
    #     plt.close()

import io
from PIL import Image

def fig_to_wandb_image(fig):
    """
    Converts a Matplotlib figure to a W&B-ready image 
    with high DPI and no excess white space.
    """
    buf = io.BytesIO()
    # bbox_inches='tight' is the magic part that removes the white margins
    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1, dpi=120)
    buf.seek(0)
    img = Image.open(buf)
    return wandb.Image(img)

def log_visualizations(model, dataloader, device, epoch):
    model.eval()
    with torch.no_grad():
        x, y = next(iter(dataloader))
        x = x.to(device)
        x_hat, _, mu, _ = model(x)
        
        # 1. Reconstruction Sample (using fig_to_wandb_image for clean borders)
        fig_reco, ax_reco = plt.subplots(1, 2, figsize=(10, 4))
        ax_reco[0].plot(x[0].cpu().numpy())
        ax_reco[0].set_title("Original Pulse")
        ax_reco[1].plot(x_hat[0].cpu().numpy(), color='orange')
        ax_reco[1].set_title("Reconstructed Pulse")
        wandb.log({"reconstruction_sample": fig_to_wandb_image(fig_reco), "epoch": epoch})
        plt.close(fig_reco)

        # 2. Dimensionality Reduction Comparison (PCA vs t-SNE)
        latent_data = mu.cpu().numpy()
        
        # Calculate PCA and t-SNE
        pca_res = PCA(n_components=2).fit_transform(latent_data)
        tsne_res = TSNE(n_components=2, random_state=42).fit_transform(latent_data)
        
        fig_dim, axes = plt.subplots(1, 2, figsize=(15, 6))
        class_labels = ['Œ≥', 'n', 'p']
        
        # Plot PCA
        scatter_pca = axes[0].scatter(pca_res[:, 0], pca_res[:, 1], c=y.numpy(), cmap='viridis', alpha=0.6)
        axes[0].set_title("PCA (Global Variance)")
        axes[0].set_xlabel("PC 1")
        axes[0].set_ylabel("PC 2")
        
        # Plot t-SNE
        scatter_tsne = axes[1].scatter(tsne_res[:, 0], tsne_res[:, 1], c=y.numpy(), cmap='viridis', alpha=0.6)
        axes[1].set_title("t-SNE (Local Clusters)")
        axes[1].set_xlabel("t-SNE 1")
        axes[1].set_ylabel("t-SNE 2")
        
        # Add a shared colorbar for both
        cbar = fig_dim.colorbar(scatter_tsne, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)
        cbar.set_label("0: Gamma, 1: Neutron, 2: Pile-up")

        wandb.log({"latent_space_comparison": fig_to_wandb_image(fig_dim), "epoch": epoch})
        plt.close(fig_dim)
